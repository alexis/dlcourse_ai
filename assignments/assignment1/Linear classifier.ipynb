{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Implement gradient check function\n",
    "def sqr(x):\n",
    "    return x*x, 2*x\n",
    "\n",
    "check_gradient(sqr, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547121"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([[-5, 0, 5]]))\n",
    "linear_classifer.cross_entropy_loss(probs, [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([[1, 0, 0]]), [1])\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([[1, 0, 0]], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=batch_size).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=batch_size).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397363\n",
      "Epoch 1, loss: 2.330354\n",
      "Epoch 2, loss: 2.311002\n",
      "Epoch 3, loss: 2.303897\n",
      "Epoch 4, loss: 2.303257\n",
      "Epoch 5, loss: 2.302898\n",
      "Epoch 6, loss: 2.302564\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301256\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde25618588>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.151\n",
      "Epoch 0, loss: 2.302725\n",
      "Epoch 1, loss: 2.302791\n",
      "Epoch 2, loss: 2.301357\n",
      "Epoch 3, loss: 2.301276\n",
      "Epoch 4, loss: 2.302099\n",
      "Epoch 5, loss: 2.301970\n",
      "Epoch 6, loss: 2.302318\n",
      "Epoch 7, loss: 2.301796\n",
      "Epoch 8, loss: 2.302969\n",
      "Epoch 9, loss: 2.301831\n",
      "Epoch 10, loss: 2.301928\n",
      "Epoch 11, loss: 2.302432\n",
      "Epoch 12, loss: 2.301526\n",
      "Epoch 13, loss: 2.301444\n",
      "Epoch 14, loss: 2.302172\n",
      "Epoch 15, loss: 2.301620\n",
      "Epoch 16, loss: 2.302301\n",
      "Epoch 17, loss: 2.301777\n",
      "Epoch 18, loss: 2.302078\n",
      "Epoch 19, loss: 2.301977\n",
      "Epoch 20, loss: 2.301629\n",
      "Epoch 21, loss: 2.302418\n",
      "Epoch 22, loss: 2.302278\n",
      "Epoch 23, loss: 2.301742\n",
      "Epoch 24, loss: 2.302121\n",
      "Epoch 25, loss: 2.301680\n",
      "Epoch 26, loss: 2.302512\n",
      "Epoch 27, loss: 2.301754\n",
      "Epoch 28, loss: 2.302120\n",
      "Epoch 29, loss: 2.302805\n",
      "Epoch 30, loss: 2.302384\n",
      "Epoch 31, loss: 2.301618\n",
      "Epoch 32, loss: 2.301947\n",
      "Epoch 33, loss: 2.301429\n",
      "Epoch 34, loss: 2.302169\n",
      "Epoch 35, loss: 2.302310\n",
      "Epoch 36, loss: 2.302223\n",
      "Epoch 37, loss: 2.302167\n",
      "Epoch 38, loss: 2.301510\n",
      "Epoch 39, loss: 2.302373\n",
      "Epoch 40, loss: 2.301593\n",
      "Epoch 41, loss: 2.302295\n",
      "Epoch 42, loss: 2.301776\n",
      "Epoch 43, loss: 2.301710\n",
      "Epoch 44, loss: 2.301632\n",
      "Epoch 45, loss: 2.302017\n",
      "Epoch 46, loss: 2.301928\n",
      "Epoch 47, loss: 2.301891\n",
      "Epoch 48, loss: 2.302249\n",
      "Epoch 49, loss: 2.302099\n",
      "Epoch 50, loss: 2.302482\n",
      "Epoch 51, loss: 2.301710\n",
      "Epoch 52, loss: 2.302051\n",
      "Epoch 53, loss: 2.301261\n",
      "Epoch 54, loss: 2.301064\n",
      "Epoch 55, loss: 2.302009\n",
      "Epoch 56, loss: 2.302295\n",
      "Epoch 57, loss: 2.302358\n",
      "Epoch 58, loss: 2.302537\n",
      "Epoch 59, loss: 2.301810\n",
      "Epoch 60, loss: 2.302019\n",
      "Epoch 61, loss: 2.301959\n",
      "Epoch 62, loss: 2.302246\n",
      "Epoch 63, loss: 2.301734\n",
      "Epoch 64, loss: 2.302239\n",
      "Epoch 65, loss: 2.302871\n",
      "Epoch 66, loss: 2.302486\n",
      "Epoch 67, loss: 2.301727\n",
      "Epoch 68, loss: 2.301941\n",
      "Epoch 69, loss: 2.301609\n",
      "Epoch 70, loss: 2.302091\n",
      "Epoch 71, loss: 2.302333\n",
      "Epoch 72, loss: 2.302013\n",
      "Epoch 73, loss: 2.302388\n",
      "Epoch 74, loss: 2.303215\n",
      "Epoch 75, loss: 2.301644\n",
      "Epoch 76, loss: 2.302333\n",
      "Epoch 77, loss: 2.302079\n",
      "Epoch 78, loss: 2.302175\n",
      "Epoch 79, loss: 2.302200\n",
      "Epoch 80, loss: 2.302681\n",
      "Epoch 81, loss: 2.302282\n",
      "Epoch 82, loss: 2.301914\n",
      "Epoch 83, loss: 2.302758\n",
      "Epoch 84, loss: 2.302432\n",
      "Epoch 85, loss: 2.301546\n",
      "Epoch 86, loss: 2.302383\n",
      "Epoch 87, loss: 2.301578\n",
      "Epoch 88, loss: 2.302560\n",
      "Epoch 89, loss: 2.302685\n",
      "Epoch 90, loss: 2.302451\n",
      "Epoch 91, loss: 2.302340\n",
      "Epoch 92, loss: 2.301760\n",
      "Epoch 93, loss: 2.301998\n",
      "Epoch 94, loss: 2.301217\n",
      "Epoch 95, loss: 2.302232\n",
      "Epoch 96, loss: 2.302160\n",
      "Epoch 97, loss: 2.301785\n",
      "Epoch 98, loss: 2.301757\n",
      "Epoch 99, loss: 2.302139\n",
      "Accuracy after training for 100 epochs:  0.143\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302072\n",
      "Epoch 1, loss: 2.301252\n",
      "Epoch 2, loss: 2.298745\n",
      "Epoch 3, loss: 2.300135\n",
      "Epoch 4, loss: 2.300572\n",
      "Epoch 5, loss: 2.296620\n",
      "Epoch 6, loss: 2.295507\n",
      "Epoch 7, loss: 2.297866\n",
      "Epoch 8, loss: 2.292782\n",
      "Epoch 9, loss: 2.296167\n",
      "Epoch 10, loss: 2.294574\n",
      "Epoch 11, loss: 2.292713\n",
      "Epoch 12, loss: 2.293354\n",
      "Epoch 13, loss: 2.291753\n",
      "Epoch 14, loss: 2.288502\n",
      "Epoch 15, loss: 2.290252\n",
      "Epoch 16, loss: 2.289301\n",
      "Epoch 17, loss: 2.284615\n",
      "Epoch 18, loss: 2.287594\n",
      "Epoch 19, loss: 2.289355\n",
      "Epoch 20, loss: 2.286312\n",
      "Epoch 21, loss: 2.287539\n",
      "Epoch 22, loss: 2.283076\n",
      "Epoch 23, loss: 2.286043\n",
      "Epoch 24, loss: 2.279854\n",
      "Epoch 25, loss: 2.283448\n",
      "Epoch 26, loss: 2.283372\n",
      "Epoch 27, loss: 2.287461\n",
      "Epoch 28, loss: 2.274858\n",
      "Epoch 29, loss: 2.279832\n",
      "Epoch 30, loss: 2.284169\n",
      "Epoch 31, loss: 2.272767\n",
      "Epoch 32, loss: 2.274322\n",
      "Epoch 33, loss: 2.272513\n",
      "Epoch 34, loss: 2.278540\n",
      "Epoch 35, loss: 2.282478\n",
      "Epoch 36, loss: 2.269291\n",
      "Epoch 37, loss: 2.272107\n",
      "Epoch 38, loss: 2.279555\n",
      "Epoch 39, loss: 2.273525\n",
      "Epoch 40, loss: 2.275780\n",
      "Epoch 41, loss: 2.283022\n",
      "Epoch 42, loss: 2.269830\n",
      "Epoch 43, loss: 2.268761\n",
      "Epoch 44, loss: 2.273783\n",
      "Epoch 45, loss: 2.268238\n",
      "Epoch 46, loss: 2.272653\n",
      "Epoch 47, loss: 2.270795\n",
      "Epoch 48, loss: 2.265578\n",
      "Epoch 49, loss: 2.278249\n",
      "Epoch 50, loss: 2.267341\n",
      "Epoch 51, loss: 2.269767\n",
      "Epoch 52, loss: 2.251338\n",
      "Epoch 53, loss: 2.253915\n",
      "Epoch 54, loss: 2.266664\n",
      "Epoch 55, loss: 2.255154\n",
      "Epoch 56, loss: 2.262607\n",
      "Epoch 57, loss: 2.270093\n",
      "Epoch 58, loss: 2.245373\n",
      "Epoch 59, loss: 2.276953\n",
      "Epoch 60, loss: 2.247248\n",
      "Epoch 61, loss: 2.271091\n",
      "Epoch 62, loss: 2.263715\n",
      "Epoch 63, loss: 2.255750\n",
      "Epoch 64, loss: 2.272063\n",
      "Epoch 65, loss: 2.253142\n",
      "Epoch 66, loss: 2.267019\n",
      "Epoch 67, loss: 2.273018\n",
      "Epoch 68, loss: 2.259054\n",
      "Epoch 69, loss: 2.256064\n",
      "Epoch 70, loss: 2.250918\n",
      "Epoch 71, loss: 2.266365\n",
      "Epoch 72, loss: 2.255286\n",
      "Epoch 73, loss: 2.259591\n",
      "Epoch 74, loss: 2.261533\n",
      "Epoch 75, loss: 2.255740\n",
      "Epoch 76, loss: 2.255460\n",
      "Epoch 77, loss: 2.246330\n",
      "Epoch 78, loss: 2.253371\n",
      "Epoch 79, loss: 2.249332\n",
      "Epoch 80, loss: 2.258319\n",
      "Epoch 81, loss: 2.253389\n",
      "Epoch 82, loss: 2.255165\n",
      "Epoch 83, loss: 2.235126\n",
      "Epoch 84, loss: 2.250329\n",
      "Epoch 85, loss: 2.245378\n",
      "Epoch 86, loss: 2.254653\n",
      "Epoch 87, loss: 2.253766\n",
      "Epoch 88, loss: 2.257267\n",
      "Epoch 89, loss: 2.253398\n",
      "Epoch 90, loss: 2.250637\n",
      "Epoch 91, loss: 2.245496\n",
      "Epoch 92, loss: 2.250213\n",
      "Epoch 93, loss: 2.255593\n",
      "Epoch 94, loss: 2.251231\n",
      "Epoch 95, loss: 2.235813\n",
      "Epoch 96, loss: 2.242285\n",
      "Epoch 97, loss: 2.239448\n",
      "Epoch 98, loss: 2.234116\n",
      "Epoch 99, loss: 2.244978\n",
      "Epoch 100, loss: 2.233775\n",
      "Epoch 101, loss: 2.243274\n",
      "Epoch 102, loss: 2.240517\n",
      "Epoch 103, loss: 2.239036\n",
      "Epoch 104, loss: 2.232201\n",
      "Epoch 105, loss: 2.231469\n",
      "Epoch 106, loss: 2.241205\n",
      "Epoch 107, loss: 2.240795\n",
      "Epoch 108, loss: 2.252071\n",
      "Epoch 109, loss: 2.231961\n",
      "Epoch 110, loss: 2.220858\n",
      "Epoch 111, loss: 2.221900\n",
      "Epoch 112, loss: 2.248359\n",
      "Epoch 113, loss: 2.262427\n",
      "Epoch 114, loss: 2.228568\n",
      "Epoch 115, loss: 2.232501\n",
      "Epoch 116, loss: 2.247062\n",
      "Epoch 117, loss: 2.219366\n",
      "Epoch 118, loss: 2.252061\n",
      "Epoch 119, loss: 2.242384\n",
      "Epoch 120, loss: 2.234422\n",
      "Epoch 121, loss: 2.215664\n",
      "Epoch 122, loss: 2.215671\n",
      "Epoch 123, loss: 2.219427\n",
      "Epoch 124, loss: 2.210522\n",
      "Epoch 125, loss: 2.221845\n",
      "Epoch 126, loss: 2.245923\n",
      "Epoch 127, loss: 2.248807\n",
      "Epoch 128, loss: 2.221614\n",
      "Epoch 129, loss: 2.222215\n",
      "Epoch 130, loss: 2.230251\n",
      "Epoch 131, loss: 2.226524\n",
      "Epoch 132, loss: 2.238553\n",
      "Epoch 133, loss: 2.241947\n",
      "Epoch 134, loss: 2.218646\n",
      "Epoch 135, loss: 2.237074\n",
      "Epoch 136, loss: 2.241551\n",
      "Epoch 137, loss: 2.236628\n",
      "Epoch 138, loss: 2.232185\n",
      "Epoch 139, loss: 2.236760\n",
      "Epoch 140, loss: 2.242615\n",
      "Epoch 141, loss: 2.230674\n",
      "Epoch 142, loss: 2.234668\n",
      "Epoch 143, loss: 2.216979\n",
      "Epoch 144, loss: 2.218433\n",
      "Epoch 145, loss: 2.245639\n",
      "Epoch 146, loss: 2.213745\n",
      "Epoch 147, loss: 2.219782\n",
      "Epoch 148, loss: 2.203413\n",
      "Epoch 149, loss: 2.210311\n",
      "Epoch 150, loss: 2.230352\n",
      "Epoch 151, loss: 2.225926\n",
      "Epoch 152, loss: 2.207366\n",
      "Epoch 153, loss: 2.206654\n",
      "Epoch 154, loss: 2.230940\n",
      "Epoch 155, loss: 2.234807\n",
      "Epoch 156, loss: 2.232130\n",
      "Epoch 157, loss: 2.225556\n",
      "Epoch 158, loss: 2.238978\n",
      "Epoch 159, loss: 2.205903\n",
      "Epoch 160, loss: 2.211431\n",
      "Epoch 161, loss: 2.202041\n",
      "Epoch 162, loss: 2.231503\n",
      "Epoch 163, loss: 2.222043\n",
      "Epoch 164, loss: 2.223041\n",
      "Epoch 165, loss: 2.222660\n",
      "Epoch 166, loss: 2.204850\n",
      "Epoch 167, loss: 2.199185\n",
      "Epoch 168, loss: 2.230927\n",
      "Epoch 169, loss: 2.221389\n",
      "Epoch 170, loss: 2.217289\n",
      "Epoch 171, loss: 2.217356\n",
      "Epoch 172, loss: 2.215196\n",
      "Epoch 173, loss: 2.192633\n",
      "Epoch 174, loss: 2.186434\n",
      "Epoch 175, loss: 2.202879\n",
      "Epoch 176, loss: 2.212009\n",
      "Epoch 177, loss: 2.218560\n",
      "Epoch 178, loss: 2.226113\n",
      "Epoch 179, loss: 2.206741\n",
      "Epoch 180, loss: 2.187552\n",
      "Epoch 181, loss: 2.228635\n",
      "Epoch 182, loss: 2.201063\n",
      "Epoch 183, loss: 2.229102\n",
      "Epoch 184, loss: 2.198408\n",
      "Epoch 185, loss: 2.202905\n",
      "Epoch 186, loss: 2.215001\n",
      "Epoch 187, loss: 2.212679\n",
      "Epoch 188, loss: 2.216987\n",
      "Epoch 189, loss: 2.200410\n",
      "Epoch 190, loss: 2.193194\n",
      "Epoch 191, loss: 2.241052\n",
      "Epoch 192, loss: 2.210209\n",
      "Epoch 193, loss: 2.213033\n",
      "Epoch 194, loss: 2.233788\n",
      "Epoch 195, loss: 2.226762\n",
      "Epoch 196, loss: 2.225477\n",
      "Epoch 197, loss: 2.203981\n",
      "Epoch 198, loss: 2.204850\n",
      "Epoch 199, loss: 2.209086\n",
      "Epoch 0, loss: 2.301440\n",
      "Epoch 1, loss: 2.300942\n",
      "Epoch 2, loss: 2.300528\n",
      "Epoch 3, loss: 2.298961\n",
      "Epoch 4, loss: 2.297432\n",
      "Epoch 5, loss: 2.301042\n",
      "Epoch 6, loss: 2.295876\n",
      "Epoch 7, loss: 2.295039\n",
      "Epoch 8, loss: 2.297211\n",
      "Epoch 9, loss: 2.298636\n",
      "Epoch 10, loss: 2.294284\n",
      "Epoch 11, loss: 2.294993\n",
      "Epoch 12, loss: 2.292331\n",
      "Epoch 13, loss: 2.295783\n",
      "Epoch 14, loss: 2.293603\n",
      "Epoch 15, loss: 2.289633\n",
      "Epoch 16, loss: 2.288130\n",
      "Epoch 17, loss: 2.283555\n",
      "Epoch 18, loss: 2.286692\n",
      "Epoch 19, loss: 2.290589\n",
      "Epoch 20, loss: 2.289793\n",
      "Epoch 21, loss: 2.290225\n",
      "Epoch 22, loss: 2.283639\n",
      "Epoch 23, loss: 2.284210\n",
      "Epoch 24, loss: 2.286540\n",
      "Epoch 25, loss: 2.282162\n",
      "Epoch 26, loss: 2.280102\n",
      "Epoch 27, loss: 2.279552\n",
      "Epoch 28, loss: 2.275863\n",
      "Epoch 29, loss: 2.283721\n",
      "Epoch 30, loss: 2.278396\n",
      "Epoch 31, loss: 2.273528\n",
      "Epoch 32, loss: 2.272920\n",
      "Epoch 33, loss: 2.273150\n",
      "Epoch 34, loss: 2.281250\n",
      "Epoch 35, loss: 2.270075\n",
      "Epoch 36, loss: 2.265482\n",
      "Epoch 37, loss: 2.280750\n",
      "Epoch 38, loss: 2.270364\n",
      "Epoch 39, loss: 2.270000\n",
      "Epoch 40, loss: 2.275630\n",
      "Epoch 41, loss: 2.258946\n",
      "Epoch 42, loss: 2.279241\n",
      "Epoch 43, loss: 2.266735\n",
      "Epoch 44, loss: 2.272526\n",
      "Epoch 45, loss: 2.266317\n",
      "Epoch 46, loss: 2.267869\n",
      "Epoch 47, loss: 2.264695\n",
      "Epoch 48, loss: 2.269671\n",
      "Epoch 49, loss: 2.269571\n",
      "Epoch 50, loss: 2.271001\n",
      "Epoch 51, loss: 2.272018\n",
      "Epoch 52, loss: 2.274956\n",
      "Epoch 53, loss: 2.257587\n",
      "Epoch 54, loss: 2.273609\n",
      "Epoch 55, loss: 2.262995\n",
      "Epoch 56, loss: 2.263020\n",
      "Epoch 57, loss: 2.264773\n",
      "Epoch 58, loss: 2.272866\n",
      "Epoch 59, loss: 2.257455\n",
      "Epoch 60, loss: 2.266678\n",
      "Epoch 61, loss: 2.243350\n",
      "Epoch 62, loss: 2.264376\n",
      "Epoch 63, loss: 2.263248\n",
      "Epoch 64, loss: 2.255815\n",
      "Epoch 65, loss: 2.262124\n",
      "Epoch 66, loss: 2.254549\n",
      "Epoch 67, loss: 2.249992\n",
      "Epoch 68, loss: 2.240249\n",
      "Epoch 69, loss: 2.252654\n",
      "Epoch 70, loss: 2.255025\n",
      "Epoch 71, loss: 2.270575\n",
      "Epoch 72, loss: 2.259987\n",
      "Epoch 73, loss: 2.266464\n",
      "Epoch 74, loss: 2.247961\n",
      "Epoch 75, loss: 2.252905\n",
      "Epoch 76, loss: 2.251599\n",
      "Epoch 77, loss: 2.253633\n",
      "Epoch 78, loss: 2.264861\n",
      "Epoch 79, loss: 2.258985\n",
      "Epoch 80, loss: 2.248254\n",
      "Epoch 81, loss: 2.252887\n",
      "Epoch 82, loss: 2.254735\n",
      "Epoch 83, loss: 2.252445\n",
      "Epoch 84, loss: 2.248267\n",
      "Epoch 85, loss: 2.251086\n",
      "Epoch 86, loss: 2.239372\n",
      "Epoch 87, loss: 2.251049\n",
      "Epoch 88, loss: 2.243962\n",
      "Epoch 89, loss: 2.251092\n",
      "Epoch 90, loss: 2.235420\n",
      "Epoch 91, loss: 2.249623\n",
      "Epoch 92, loss: 2.244286\n",
      "Epoch 93, loss: 2.233458\n",
      "Epoch 94, loss: 2.243953\n",
      "Epoch 95, loss: 2.235361\n",
      "Epoch 96, loss: 2.233329\n",
      "Epoch 97, loss: 2.235186\n",
      "Epoch 98, loss: 2.247327\n",
      "Epoch 99, loss: 2.248688\n",
      "Epoch 100, loss: 2.237610\n",
      "Epoch 101, loss: 2.245505\n",
      "Epoch 102, loss: 2.249228\n",
      "Epoch 103, loss: 2.232137\n",
      "Epoch 104, loss: 2.236398\n",
      "Epoch 105, loss: 2.243840\n",
      "Epoch 106, loss: 2.232630\n",
      "Epoch 107, loss: 2.248223\n",
      "Epoch 108, loss: 2.242945\n",
      "Epoch 109, loss: 2.248484\n",
      "Epoch 110, loss: 2.227502\n",
      "Epoch 111, loss: 2.227975\n",
      "Epoch 112, loss: 2.246012\n",
      "Epoch 113, loss: 2.241169\n",
      "Epoch 114, loss: 2.239560\n",
      "Epoch 115, loss: 2.254008\n",
      "Epoch 116, loss: 2.251347\n",
      "Epoch 117, loss: 2.251307\n",
      "Epoch 118, loss: 2.240221\n",
      "Epoch 119, loss: 2.237808\n",
      "Epoch 120, loss: 2.218093\n",
      "Epoch 121, loss: 2.251301\n",
      "Epoch 122, loss: 2.236328\n",
      "Epoch 123, loss: 2.214901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, loss: 2.234536\n",
      "Epoch 125, loss: 2.227084\n",
      "Epoch 126, loss: 2.227421\n",
      "Epoch 127, loss: 2.221553\n",
      "Epoch 128, loss: 2.212068\n",
      "Epoch 129, loss: 2.235508\n",
      "Epoch 130, loss: 2.238055\n",
      "Epoch 131, loss: 2.235050\n",
      "Epoch 132, loss: 2.225560\n",
      "Epoch 133, loss: 2.214029\n",
      "Epoch 134, loss: 2.234035\n",
      "Epoch 135, loss: 2.243711\n",
      "Epoch 136, loss: 2.218825\n",
      "Epoch 137, loss: 2.222974\n",
      "Epoch 138, loss: 2.195035\n",
      "Epoch 139, loss: 2.234761\n",
      "Epoch 140, loss: 2.251964\n",
      "Epoch 141, loss: 2.216085\n",
      "Epoch 142, loss: 2.229043\n",
      "Epoch 143, loss: 2.222980\n",
      "Epoch 144, loss: 2.202988\n",
      "Epoch 145, loss: 2.244413\n",
      "Epoch 146, loss: 2.215938\n",
      "Epoch 147, loss: 2.235343\n",
      "Epoch 148, loss: 2.230762\n",
      "Epoch 149, loss: 2.236344\n",
      "Epoch 150, loss: 2.233491\n",
      "Epoch 151, loss: 2.228373\n",
      "Epoch 152, loss: 2.209909\n",
      "Epoch 153, loss: 2.209120\n",
      "Epoch 154, loss: 2.207140\n",
      "Epoch 155, loss: 2.211965\n",
      "Epoch 156, loss: 2.213359\n",
      "Epoch 157, loss: 2.220014\n",
      "Epoch 158, loss: 2.219101\n",
      "Epoch 159, loss: 2.237951\n",
      "Epoch 160, loss: 2.227035\n",
      "Epoch 161, loss: 2.216053\n",
      "Epoch 162, loss: 2.221420\n",
      "Epoch 163, loss: 2.219066\n",
      "Epoch 164, loss: 2.227193\n",
      "Epoch 165, loss: 2.200039\n",
      "Epoch 166, loss: 2.227480\n",
      "Epoch 167, loss: 2.219327\n",
      "Epoch 168, loss: 2.227234\n",
      "Epoch 169, loss: 2.234041\n",
      "Epoch 170, loss: 2.239519\n",
      "Epoch 171, loss: 2.220330\n",
      "Epoch 172, loss: 2.200833\n",
      "Epoch 173, loss: 2.219759\n",
      "Epoch 174, loss: 2.217235\n",
      "Epoch 175, loss: 2.232738\n",
      "Epoch 176, loss: 2.215931\n",
      "Epoch 177, loss: 2.221353\n",
      "Epoch 178, loss: 2.214907\n",
      "Epoch 179, loss: 2.220101\n",
      "Epoch 180, loss: 2.217344\n",
      "Epoch 181, loss: 2.233324\n",
      "Epoch 182, loss: 2.224630\n",
      "Epoch 183, loss: 2.207651\n",
      "Epoch 184, loss: 2.223822\n",
      "Epoch 185, loss: 2.214770\n",
      "Epoch 186, loss: 2.200070\n",
      "Epoch 187, loss: 2.218315\n",
      "Epoch 188, loss: 2.191454\n",
      "Epoch 189, loss: 2.208849\n",
      "Epoch 190, loss: 2.204132\n",
      "Epoch 191, loss: 2.178042\n",
      "Epoch 192, loss: 2.202692\n",
      "Epoch 193, loss: 2.178042\n",
      "Epoch 194, loss: 2.209496\n",
      "Epoch 195, loss: 2.207650\n",
      "Epoch 196, loss: 2.218699\n",
      "Epoch 197, loss: 2.209245\n",
      "Epoch 198, loss: 2.218860\n",
      "Epoch 199, loss: 2.185558\n",
      "Epoch 0, loss: 2.301602\n",
      "Epoch 1, loss: 2.301641\n",
      "Epoch 2, loss: 2.298846\n",
      "Epoch 3, loss: 2.300636\n",
      "Epoch 4, loss: 2.297194\n",
      "Epoch 5, loss: 2.297649\n",
      "Epoch 6, loss: 2.295963\n",
      "Epoch 7, loss: 2.295306\n",
      "Epoch 8, loss: 2.293715\n",
      "Epoch 9, loss: 2.295505\n",
      "Epoch 10, loss: 2.294821\n",
      "Epoch 11, loss: 2.289950\n",
      "Epoch 12, loss: 2.294059\n",
      "Epoch 13, loss: 2.290613\n",
      "Epoch 14, loss: 2.289583\n",
      "Epoch 15, loss: 2.289439\n",
      "Epoch 16, loss: 2.288843\n",
      "Epoch 17, loss: 2.287031\n",
      "Epoch 18, loss: 2.285772\n",
      "Epoch 19, loss: 2.290436\n",
      "Epoch 20, loss: 2.286122\n",
      "Epoch 21, loss: 2.288596\n",
      "Epoch 22, loss: 2.284995\n",
      "Epoch 23, loss: 2.288782\n",
      "Epoch 24, loss: 2.286321\n",
      "Epoch 25, loss: 2.285741\n",
      "Epoch 26, loss: 2.284549\n",
      "Epoch 27, loss: 2.283934\n",
      "Epoch 28, loss: 2.283635\n",
      "Epoch 29, loss: 2.287635\n",
      "Epoch 30, loss: 2.276916\n",
      "Epoch 31, loss: 2.279353\n",
      "Epoch 32, loss: 2.281698\n",
      "Epoch 33, loss: 2.271525\n",
      "Epoch 34, loss: 2.279939\n",
      "Epoch 35, loss: 2.271767\n",
      "Epoch 36, loss: 2.274979\n",
      "Epoch 37, loss: 2.270620\n",
      "Epoch 38, loss: 2.282922\n",
      "Epoch 39, loss: 2.273949\n",
      "Epoch 40, loss: 2.276380\n",
      "Epoch 41, loss: 2.273284\n",
      "Epoch 42, loss: 2.270362\n",
      "Epoch 43, loss: 2.273047\n",
      "Epoch 44, loss: 2.264095\n",
      "Epoch 45, loss: 2.273876\n",
      "Epoch 46, loss: 2.262677\n",
      "Epoch 47, loss: 2.279388\n",
      "Epoch 48, loss: 2.259271\n",
      "Epoch 49, loss: 2.264749\n",
      "Epoch 50, loss: 2.268550\n",
      "Epoch 51, loss: 2.271021\n",
      "Epoch 52, loss: 2.271238\n",
      "Epoch 53, loss: 2.269548\n",
      "Epoch 54, loss: 2.266414\n",
      "Epoch 55, loss: 2.269100\n",
      "Epoch 56, loss: 2.260252\n",
      "Epoch 57, loss: 2.263770\n",
      "Epoch 58, loss: 2.268088\n",
      "Epoch 59, loss: 2.269529\n",
      "Epoch 60, loss: 2.260587\n",
      "Epoch 61, loss: 2.260618\n",
      "Epoch 62, loss: 2.257757\n",
      "Epoch 63, loss: 2.260181\n",
      "Epoch 64, loss: 2.254149\n",
      "Epoch 65, loss: 2.250086\n",
      "Epoch 66, loss: 2.251142\n",
      "Epoch 67, loss: 2.241640\n",
      "Epoch 68, loss: 2.249928\n",
      "Epoch 69, loss: 2.263655\n",
      "Epoch 70, loss: 2.244032\n",
      "Epoch 71, loss: 2.264747\n",
      "Epoch 72, loss: 2.260927\n",
      "Epoch 73, loss: 2.257331\n",
      "Epoch 74, loss: 2.255372\n",
      "Epoch 75, loss: 2.259109\n",
      "Epoch 76, loss: 2.252401\n",
      "Epoch 77, loss: 2.248799\n",
      "Epoch 78, loss: 2.248594\n",
      "Epoch 79, loss: 2.236292\n",
      "Epoch 80, loss: 2.248858\n",
      "Epoch 81, loss: 2.254903\n",
      "Epoch 82, loss: 2.246902\n",
      "Epoch 83, loss: 2.251356\n",
      "Epoch 84, loss: 2.263660\n",
      "Epoch 85, loss: 2.253711\n",
      "Epoch 86, loss: 2.249813\n",
      "Epoch 87, loss: 2.234777\n",
      "Epoch 88, loss: 2.259513\n",
      "Epoch 89, loss: 2.243288\n",
      "Epoch 90, loss: 2.250462\n",
      "Epoch 91, loss: 2.256864\n",
      "Epoch 92, loss: 2.249256\n",
      "Epoch 93, loss: 2.242275\n",
      "Epoch 94, loss: 2.243593\n",
      "Epoch 95, loss: 2.258958\n",
      "Epoch 96, loss: 2.254818\n",
      "Epoch 97, loss: 2.248721\n",
      "Epoch 98, loss: 2.238922\n",
      "Epoch 99, loss: 2.230720\n",
      "Epoch 100, loss: 2.240988\n",
      "Epoch 101, loss: 2.260012\n",
      "Epoch 102, loss: 2.234735\n",
      "Epoch 103, loss: 2.258084\n",
      "Epoch 104, loss: 2.238516\n",
      "Epoch 105, loss: 2.237246\n",
      "Epoch 106, loss: 2.247936\n",
      "Epoch 107, loss: 2.235475\n",
      "Epoch 108, loss: 2.250345\n",
      "Epoch 109, loss: 2.236743\n",
      "Epoch 110, loss: 2.250425\n",
      "Epoch 111, loss: 2.231023\n",
      "Epoch 112, loss: 2.252351\n",
      "Epoch 113, loss: 2.228399\n",
      "Epoch 114, loss: 2.237573\n",
      "Epoch 115, loss: 2.241472\n",
      "Epoch 116, loss: 2.244808\n",
      "Epoch 117, loss: 2.241635\n",
      "Epoch 118, loss: 2.235031\n",
      "Epoch 119, loss: 2.224103\n",
      "Epoch 120, loss: 2.243916\n",
      "Epoch 121, loss: 2.231827\n",
      "Epoch 122, loss: 2.249887\n",
      "Epoch 123, loss: 2.220603\n",
      "Epoch 124, loss: 2.234808\n",
      "Epoch 125, loss: 2.223575\n",
      "Epoch 126, loss: 2.243005\n",
      "Epoch 127, loss: 2.236355\n",
      "Epoch 128, loss: 2.210949\n",
      "Epoch 129, loss: 2.218162\n",
      "Epoch 130, loss: 2.214617\n",
      "Epoch 131, loss: 2.218656\n",
      "Epoch 132, loss: 2.230647\n",
      "Epoch 133, loss: 2.229229\n",
      "Epoch 134, loss: 2.239875\n",
      "Epoch 135, loss: 2.222323\n",
      "Epoch 136, loss: 2.220349\n",
      "Epoch 137, loss: 2.223821\n",
      "Epoch 138, loss: 2.223268\n",
      "Epoch 139, loss: 2.228891\n",
      "Epoch 140, loss: 2.239386\n",
      "Epoch 141, loss: 2.223477\n",
      "Epoch 142, loss: 2.235251\n",
      "Epoch 143, loss: 2.223485\n",
      "Epoch 144, loss: 2.228860\n",
      "Epoch 145, loss: 2.228892\n",
      "Epoch 146, loss: 2.240450\n",
      "Epoch 147, loss: 2.206861\n",
      "Epoch 148, loss: 2.230683\n",
      "Epoch 149, loss: 2.223581\n",
      "Epoch 150, loss: 2.245066\n",
      "Epoch 151, loss: 2.222392\n",
      "Epoch 152, loss: 2.222884\n",
      "Epoch 153, loss: 2.227788\n",
      "Epoch 154, loss: 2.218283\n",
      "Epoch 155, loss: 2.218419\n",
      "Epoch 156, loss: 2.248210\n",
      "Epoch 157, loss: 2.212358\n",
      "Epoch 158, loss: 2.202524\n",
      "Epoch 159, loss: 2.220119\n",
      "Epoch 160, loss: 2.210195\n",
      "Epoch 161, loss: 2.208278\n",
      "Epoch 162, loss: 2.238418\n",
      "Epoch 163, loss: 2.224629\n",
      "Epoch 164, loss: 2.203550\n",
      "Epoch 165, loss: 2.239276\n",
      "Epoch 166, loss: 2.223794\n",
      "Epoch 167, loss: 2.209911\n",
      "Epoch 168, loss: 2.211198\n",
      "Epoch 169, loss: 2.215321\n",
      "Epoch 170, loss: 2.207717\n",
      "Epoch 171, loss: 2.234915\n",
      "Epoch 172, loss: 2.227528\n",
      "Epoch 173, loss: 2.208601\n",
      "Epoch 174, loss: 2.210646\n",
      "Epoch 175, loss: 2.216642\n",
      "Epoch 176, loss: 2.225517\n",
      "Epoch 177, loss: 2.233010\n",
      "Epoch 178, loss: 2.199329\n",
      "Epoch 179, loss: 2.217717\n",
      "Epoch 180, loss: 2.234949\n",
      "Epoch 181, loss: 2.211473\n",
      "Epoch 182, loss: 2.225826\n",
      "Epoch 183, loss: 2.188309\n",
      "Epoch 184, loss: 2.198035\n",
      "Epoch 185, loss: 2.220181\n",
      "Epoch 186, loss: 2.213574\n",
      "Epoch 187, loss: 2.211718\n",
      "Epoch 188, loss: 2.197796\n",
      "Epoch 189, loss: 2.213462\n",
      "Epoch 190, loss: 2.201578\n",
      "Epoch 191, loss: 2.181812\n",
      "Epoch 192, loss: 2.171424\n",
      "Epoch 193, loss: 2.204038\n",
      "Epoch 194, loss: 2.204641\n",
      "Epoch 195, loss: 2.212500\n",
      "Epoch 196, loss: 2.207574\n",
      "Epoch 197, loss: 2.228536\n",
      "Epoch 198, loss: 2.206288\n",
      "Epoch 199, loss: 2.223422\n",
      "Epoch 0, loss: 2.302635\n",
      "Epoch 1, loss: 2.301990\n",
      "Epoch 2, loss: 2.302755\n",
      "Epoch 3, loss: 2.302513\n",
      "Epoch 4, loss: 2.301596\n",
      "Epoch 5, loss: 2.302559\n",
      "Epoch 6, loss: 2.302369\n",
      "Epoch 7, loss: 2.302556\n",
      "Epoch 8, loss: 2.301227\n",
      "Epoch 9, loss: 2.301502\n",
      "Epoch 10, loss: 2.301602\n",
      "Epoch 11, loss: 2.301409\n",
      "Epoch 12, loss: 2.300813\n",
      "Epoch 13, loss: 2.301560\n",
      "Epoch 14, loss: 2.301900\n",
      "Epoch 15, loss: 2.301749\n",
      "Epoch 16, loss: 2.299936\n",
      "Epoch 17, loss: 2.302292\n",
      "Epoch 18, loss: 2.300118\n",
      "Epoch 19, loss: 2.300501\n",
      "Epoch 20, loss: 2.301120\n",
      "Epoch 21, loss: 2.301399\n",
      "Epoch 22, loss: 2.299779\n",
      "Epoch 23, loss: 2.299839\n",
      "Epoch 24, loss: 2.300641\n",
      "Epoch 25, loss: 2.300800\n",
      "Epoch 26, loss: 2.301176\n",
      "Epoch 27, loss: 2.300606\n",
      "Epoch 28, loss: 2.301366\n",
      "Epoch 29, loss: 2.300317\n",
      "Epoch 30, loss: 2.299503\n",
      "Epoch 31, loss: 2.299949\n",
      "Epoch 32, loss: 2.299101\n",
      "Epoch 33, loss: 2.297700\n",
      "Epoch 34, loss: 2.299277\n",
      "Epoch 35, loss: 2.300847\n",
      "Epoch 36, loss: 2.297149\n",
      "Epoch 37, loss: 2.298293\n",
      "Epoch 38, loss: 2.301752\n",
      "Epoch 39, loss: 2.299429\n",
      "Epoch 40, loss: 2.299511\n",
      "Epoch 41, loss: 2.299357\n",
      "Epoch 42, loss: 2.298287\n",
      "Epoch 43, loss: 2.296888\n",
      "Epoch 44, loss: 2.297615\n",
      "Epoch 45, loss: 2.299569\n",
      "Epoch 46, loss: 2.298703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, loss: 2.298353\n",
      "Epoch 48, loss: 2.298201\n",
      "Epoch 49, loss: 2.298952\n",
      "Epoch 50, loss: 2.299585\n",
      "Epoch 51, loss: 2.299979\n",
      "Epoch 52, loss: 2.295972\n",
      "Epoch 53, loss: 2.296419\n",
      "Epoch 54, loss: 2.296676\n",
      "Epoch 55, loss: 2.299275\n",
      "Epoch 56, loss: 2.296703\n",
      "Epoch 57, loss: 2.300092\n",
      "Epoch 58, loss: 2.298896\n",
      "Epoch 59, loss: 2.296449\n",
      "Epoch 60, loss: 2.299008\n",
      "Epoch 61, loss: 2.297185\n",
      "Epoch 62, loss: 2.296383\n",
      "Epoch 63, loss: 2.297521\n",
      "Epoch 64, loss: 2.295990\n",
      "Epoch 65, loss: 2.298930\n",
      "Epoch 66, loss: 2.298239\n",
      "Epoch 67, loss: 2.295391\n",
      "Epoch 68, loss: 2.297550\n",
      "Epoch 69, loss: 2.298679\n",
      "Epoch 70, loss: 2.297746\n",
      "Epoch 71, loss: 2.298239\n",
      "Epoch 72, loss: 2.296278\n",
      "Epoch 73, loss: 2.295000\n",
      "Epoch 74, loss: 2.297676\n",
      "Epoch 75, loss: 2.295578\n",
      "Epoch 76, loss: 2.296751\n",
      "Epoch 77, loss: 2.298036\n",
      "Epoch 78, loss: 2.295497\n",
      "Epoch 79, loss: 2.295390\n",
      "Epoch 80, loss: 2.296022\n",
      "Epoch 81, loss: 2.296421\n",
      "Epoch 82, loss: 2.293863\n",
      "Epoch 83, loss: 2.296219\n",
      "Epoch 84, loss: 2.296920\n",
      "Epoch 85, loss: 2.297250\n",
      "Epoch 86, loss: 2.293474\n",
      "Epoch 87, loss: 2.293630\n",
      "Epoch 88, loss: 2.294795\n",
      "Epoch 89, loss: 2.292582\n",
      "Epoch 90, loss: 2.293794\n",
      "Epoch 91, loss: 2.292604\n",
      "Epoch 92, loss: 2.296298\n",
      "Epoch 93, loss: 2.293853\n",
      "Epoch 94, loss: 2.296505\n",
      "Epoch 95, loss: 2.294469\n",
      "Epoch 96, loss: 2.292545\n",
      "Epoch 97, loss: 2.298952\n",
      "Epoch 98, loss: 2.291807\n",
      "Epoch 99, loss: 2.293681\n",
      "Epoch 100, loss: 2.295381\n",
      "Epoch 101, loss: 2.292771\n",
      "Epoch 102, loss: 2.298128\n",
      "Epoch 103, loss: 2.291481\n",
      "Epoch 104, loss: 2.294810\n",
      "Epoch 105, loss: 2.292305\n",
      "Epoch 106, loss: 2.294532\n",
      "Epoch 107, loss: 2.293916\n",
      "Epoch 108, loss: 2.291828\n",
      "Epoch 109, loss: 2.294952\n",
      "Epoch 110, loss: 2.290242\n",
      "Epoch 111, loss: 2.295772\n",
      "Epoch 112, loss: 2.296992\n",
      "Epoch 113, loss: 2.291722\n",
      "Epoch 114, loss: 2.295084\n",
      "Epoch 115, loss: 2.288765\n",
      "Epoch 116, loss: 2.297270\n",
      "Epoch 117, loss: 2.293325\n",
      "Epoch 118, loss: 2.292064\n",
      "Epoch 119, loss: 2.292089\n",
      "Epoch 120, loss: 2.292386\n",
      "Epoch 121, loss: 2.294341\n",
      "Epoch 122, loss: 2.292121\n",
      "Epoch 123, loss: 2.295695\n",
      "Epoch 124, loss: 2.292323\n",
      "Epoch 125, loss: 2.290649\n",
      "Epoch 126, loss: 2.292335\n",
      "Epoch 127, loss: 2.289902\n",
      "Epoch 128, loss: 2.291466\n",
      "Epoch 129, loss: 2.288064\n",
      "Epoch 130, loss: 2.288831\n",
      "Epoch 131, loss: 2.291201\n",
      "Epoch 132, loss: 2.292913\n",
      "Epoch 133, loss: 2.291444\n",
      "Epoch 134, loss: 2.291850\n",
      "Epoch 135, loss: 2.293831\n",
      "Epoch 136, loss: 2.293882\n",
      "Epoch 137, loss: 2.290315\n",
      "Epoch 138, loss: 2.289971\n",
      "Epoch 139, loss: 2.288972\n",
      "Epoch 140, loss: 2.288877\n",
      "Epoch 141, loss: 2.286695\n",
      "Epoch 142, loss: 2.290437\n",
      "Epoch 143, loss: 2.284346\n",
      "Epoch 144, loss: 2.293261\n",
      "Epoch 145, loss: 2.287899\n",
      "Epoch 146, loss: 2.292782\n",
      "Epoch 147, loss: 2.288089\n",
      "Epoch 148, loss: 2.287961\n",
      "Epoch 149, loss: 2.288920\n",
      "Epoch 150, loss: 2.292431\n",
      "Epoch 151, loss: 2.292499\n",
      "Epoch 152, loss: 2.288655\n",
      "Epoch 153, loss: 2.288199\n",
      "Epoch 154, loss: 2.290680\n",
      "Epoch 155, loss: 2.289489\n",
      "Epoch 156, loss: 2.286673\n",
      "Epoch 157, loss: 2.292210\n",
      "Epoch 158, loss: 2.287380\n",
      "Epoch 159, loss: 2.290331\n",
      "Epoch 160, loss: 2.292557\n",
      "Epoch 161, loss: 2.291529\n",
      "Epoch 162, loss: 2.290576\n",
      "Epoch 163, loss: 2.290288\n",
      "Epoch 164, loss: 2.294526\n",
      "Epoch 165, loss: 2.290772\n",
      "Epoch 166, loss: 2.292499\n",
      "Epoch 167, loss: 2.291868\n",
      "Epoch 168, loss: 2.284725\n",
      "Epoch 169, loss: 2.289765\n",
      "Epoch 170, loss: 2.288137\n",
      "Epoch 171, loss: 2.287181\n",
      "Epoch 172, loss: 2.289855\n",
      "Epoch 173, loss: 2.292036\n",
      "Epoch 174, loss: 2.287435\n",
      "Epoch 175, loss: 2.289820\n",
      "Epoch 176, loss: 2.290241\n",
      "Epoch 177, loss: 2.292139\n",
      "Epoch 178, loss: 2.286299\n",
      "Epoch 179, loss: 2.288525\n",
      "Epoch 180, loss: 2.288111\n",
      "Epoch 181, loss: 2.286545\n",
      "Epoch 182, loss: 2.290277\n",
      "Epoch 183, loss: 2.286463\n",
      "Epoch 184, loss: 2.288620\n",
      "Epoch 185, loss: 2.289716\n",
      "Epoch 186, loss: 2.283719\n",
      "Epoch 187, loss: 2.285401\n",
      "Epoch 188, loss: 2.286044\n",
      "Epoch 189, loss: 2.285157\n",
      "Epoch 190, loss: 2.280084\n",
      "Epoch 191, loss: 2.284282\n",
      "Epoch 192, loss: 2.287423\n",
      "Epoch 193, loss: 2.286247\n",
      "Epoch 194, loss: 2.291450\n",
      "Epoch 195, loss: 2.283805\n",
      "Epoch 196, loss: 2.290476\n",
      "Epoch 197, loss: 2.286706\n",
      "Epoch 198, loss: 2.284146\n",
      "Epoch 199, loss: 2.287375\n",
      "Epoch 0, loss: 2.303400\n",
      "Epoch 1, loss: 2.303308\n",
      "Epoch 2, loss: 2.303135\n",
      "Epoch 3, loss: 2.302373\n",
      "Epoch 4, loss: 2.301311\n",
      "Epoch 5, loss: 2.302680\n",
      "Epoch 6, loss: 2.302640\n",
      "Epoch 7, loss: 2.302090\n",
      "Epoch 8, loss: 2.301869\n",
      "Epoch 9, loss: 2.301807\n",
      "Epoch 10, loss: 2.302109\n",
      "Epoch 11, loss: 2.302216\n",
      "Epoch 12, loss: 2.301670\n",
      "Epoch 13, loss: 2.301358\n",
      "Epoch 14, loss: 2.301740\n",
      "Epoch 15, loss: 2.301553\n",
      "Epoch 16, loss: 2.300100\n",
      "Epoch 17, loss: 2.301376\n",
      "Epoch 18, loss: 2.301579\n",
      "Epoch 19, loss: 2.300266\n",
      "Epoch 20, loss: 2.301324\n",
      "Epoch 21, loss: 2.300675\n",
      "Epoch 22, loss: 2.299778\n",
      "Epoch 23, loss: 2.299999\n",
      "Epoch 24, loss: 2.299512\n",
      "Epoch 25, loss: 2.300599\n",
      "Epoch 26, loss: 2.299415\n",
      "Epoch 27, loss: 2.300627\n",
      "Epoch 28, loss: 2.301110\n",
      "Epoch 29, loss: 2.299715\n",
      "Epoch 30, loss: 2.300160\n",
      "Epoch 31, loss: 2.300930\n",
      "Epoch 32, loss: 2.300683\n",
      "Epoch 33, loss: 2.299244\n",
      "Epoch 34, loss: 2.299959\n",
      "Epoch 35, loss: 2.299191\n",
      "Epoch 36, loss: 2.300078\n",
      "Epoch 37, loss: 2.298817\n",
      "Epoch 38, loss: 2.300460\n",
      "Epoch 39, loss: 2.297799\n",
      "Epoch 40, loss: 2.297973\n",
      "Epoch 41, loss: 2.298815\n",
      "Epoch 42, loss: 2.296075\n",
      "Epoch 43, loss: 2.299183\n",
      "Epoch 44, loss: 2.298986\n",
      "Epoch 45, loss: 2.298940\n",
      "Epoch 46, loss: 2.299033\n",
      "Epoch 47, loss: 2.298998\n",
      "Epoch 48, loss: 2.298842\n",
      "Epoch 49, loss: 2.299095\n",
      "Epoch 50, loss: 2.298629\n",
      "Epoch 51, loss: 2.298815\n",
      "Epoch 52, loss: 2.299604\n",
      "Epoch 53, loss: 2.297188\n",
      "Epoch 54, loss: 2.299013\n",
      "Epoch 55, loss: 2.299589\n",
      "Epoch 56, loss: 2.297847\n",
      "Epoch 57, loss: 2.296635\n",
      "Epoch 58, loss: 2.298320\n",
      "Epoch 59, loss: 2.297408\n",
      "Epoch 60, loss: 2.299251\n",
      "Epoch 61, loss: 2.298550\n",
      "Epoch 62, loss: 2.294731\n",
      "Epoch 63, loss: 2.295521\n",
      "Epoch 64, loss: 2.295107\n",
      "Epoch 65, loss: 2.295621\n",
      "Epoch 66, loss: 2.297255\n",
      "Epoch 67, loss: 2.296359\n",
      "Epoch 68, loss: 2.297338\n",
      "Epoch 69, loss: 2.297447\n",
      "Epoch 70, loss: 2.294020\n",
      "Epoch 71, loss: 2.296973\n",
      "Epoch 72, loss: 2.294619\n",
      "Epoch 73, loss: 2.298510\n",
      "Epoch 74, loss: 2.295100\n",
      "Epoch 75, loss: 2.297804\n",
      "Epoch 76, loss: 2.296508\n",
      "Epoch 77, loss: 2.297321\n",
      "Epoch 78, loss: 2.294328\n",
      "Epoch 79, loss: 2.293394\n",
      "Epoch 80, loss: 2.295558\n",
      "Epoch 81, loss: 2.294687\n",
      "Epoch 82, loss: 2.293521\n",
      "Epoch 83, loss: 2.297240\n",
      "Epoch 84, loss: 2.295666\n",
      "Epoch 85, loss: 2.292966\n",
      "Epoch 86, loss: 2.296420\n",
      "Epoch 87, loss: 2.298348\n",
      "Epoch 88, loss: 2.293222\n",
      "Epoch 89, loss: 2.298137\n",
      "Epoch 90, loss: 2.293607\n",
      "Epoch 91, loss: 2.293803\n",
      "Epoch 92, loss: 2.295895\n",
      "Epoch 93, loss: 2.296692\n",
      "Epoch 94, loss: 2.294578\n",
      "Epoch 95, loss: 2.292208\n",
      "Epoch 96, loss: 2.296836\n",
      "Epoch 97, loss: 2.294859\n",
      "Epoch 98, loss: 2.293752\n",
      "Epoch 99, loss: 2.292441\n",
      "Epoch 100, loss: 2.299222\n",
      "Epoch 101, loss: 2.290387\n",
      "Epoch 102, loss: 2.294179\n",
      "Epoch 103, loss: 2.294112\n",
      "Epoch 104, loss: 2.294228\n",
      "Epoch 105, loss: 2.293542\n",
      "Epoch 106, loss: 2.296609\n",
      "Epoch 107, loss: 2.295417\n",
      "Epoch 108, loss: 2.290454\n",
      "Epoch 109, loss: 2.290919\n",
      "Epoch 110, loss: 2.293927\n",
      "Epoch 111, loss: 2.296438\n",
      "Epoch 112, loss: 2.290490\n",
      "Epoch 113, loss: 2.291977\n",
      "Epoch 114, loss: 2.292114\n",
      "Epoch 115, loss: 2.294950\n",
      "Epoch 116, loss: 2.292076\n",
      "Epoch 117, loss: 2.292200\n",
      "Epoch 118, loss: 2.295014\n",
      "Epoch 119, loss: 2.292588\n",
      "Epoch 120, loss: 2.293297\n",
      "Epoch 121, loss: 2.296023\n",
      "Epoch 122, loss: 2.290824\n",
      "Epoch 123, loss: 2.294731\n",
      "Epoch 124, loss: 2.294803\n",
      "Epoch 125, loss: 2.294203\n",
      "Epoch 126, loss: 2.288674\n",
      "Epoch 127, loss: 2.291929\n",
      "Epoch 128, loss: 2.289483\n",
      "Epoch 129, loss: 2.291721\n",
      "Epoch 130, loss: 2.293108\n",
      "Epoch 131, loss: 2.290243\n",
      "Epoch 132, loss: 2.293489\n",
      "Epoch 133, loss: 2.292526\n",
      "Epoch 134, loss: 2.293859\n",
      "Epoch 135, loss: 2.289273\n",
      "Epoch 136, loss: 2.291387\n",
      "Epoch 137, loss: 2.294815\n",
      "Epoch 138, loss: 2.293597\n",
      "Epoch 139, loss: 2.291097\n",
      "Epoch 140, loss: 2.284613\n",
      "Epoch 141, loss: 2.287790\n",
      "Epoch 142, loss: 2.291115\n",
      "Epoch 143, loss: 2.291441\n",
      "Epoch 144, loss: 2.291737\n",
      "Epoch 145, loss: 2.288800\n",
      "Epoch 146, loss: 2.286448\n",
      "Epoch 147, loss: 2.294517\n",
      "Epoch 148, loss: 2.290973\n",
      "Epoch 149, loss: 2.289648\n",
      "Epoch 150, loss: 2.292057\n",
      "Epoch 151, loss: 2.286816\n",
      "Epoch 152, loss: 2.291003\n",
      "Epoch 153, loss: 2.288752\n",
      "Epoch 154, loss: 2.288218\n",
      "Epoch 155, loss: 2.290414\n",
      "Epoch 156, loss: 2.288014\n",
      "Epoch 157, loss: 2.292281\n",
      "Epoch 158, loss: 2.288958\n",
      "Epoch 159, loss: 2.290484\n",
      "Epoch 160, loss: 2.290487\n",
      "Epoch 161, loss: 2.291895\n",
      "Epoch 162, loss: 2.287495\n",
      "Epoch 163, loss: 2.289242\n",
      "Epoch 164, loss: 2.291242\n",
      "Epoch 165, loss: 2.286312\n",
      "Epoch 166, loss: 2.290847\n",
      "Epoch 167, loss: 2.284852\n",
      "Epoch 168, loss: 2.288464\n",
      "Epoch 169, loss: 2.287828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170, loss: 2.288999\n",
      "Epoch 171, loss: 2.286994\n",
      "Epoch 172, loss: 2.290162\n",
      "Epoch 173, loss: 2.288366\n",
      "Epoch 174, loss: 2.288863\n",
      "Epoch 175, loss: 2.284760\n",
      "Epoch 176, loss: 2.291873\n",
      "Epoch 177, loss: 2.290572\n",
      "Epoch 178, loss: 2.288893\n",
      "Epoch 179, loss: 2.291316\n",
      "Epoch 180, loss: 2.288525\n",
      "Epoch 181, loss: 2.285751\n",
      "Epoch 182, loss: 2.283730\n",
      "Epoch 183, loss: 2.288425\n",
      "Epoch 184, loss: 2.287366\n",
      "Epoch 185, loss: 2.285907\n",
      "Epoch 186, loss: 2.289882\n",
      "Epoch 187, loss: 2.290835\n",
      "Epoch 188, loss: 2.290970\n",
      "Epoch 189, loss: 2.290120\n",
      "Epoch 190, loss: 2.287258\n",
      "Epoch 191, loss: 2.285749\n",
      "Epoch 192, loss: 2.282709\n",
      "Epoch 193, loss: 2.288870\n",
      "Epoch 194, loss: 2.286760\n",
      "Epoch 195, loss: 2.286847\n",
      "Epoch 196, loss: 2.284786\n",
      "Epoch 197, loss: 2.287357\n",
      "Epoch 198, loss: 2.290482\n",
      "Epoch 199, loss: 2.282610\n",
      "Epoch 0, loss: 2.303970\n",
      "Epoch 1, loss: 2.301961\n",
      "Epoch 2, loss: 2.303042\n",
      "Epoch 3, loss: 2.302859\n",
      "Epoch 4, loss: 2.301889\n",
      "Epoch 5, loss: 2.301754\n",
      "Epoch 6, loss: 2.302501\n",
      "Epoch 7, loss: 2.301970\n",
      "Epoch 8, loss: 2.302477\n",
      "Epoch 9, loss: 2.301318\n",
      "Epoch 10, loss: 2.301763\n",
      "Epoch 11, loss: 2.301528\n",
      "Epoch 12, loss: 2.300094\n",
      "Epoch 13, loss: 2.301575\n",
      "Epoch 14, loss: 2.300433\n",
      "Epoch 15, loss: 2.301302\n",
      "Epoch 16, loss: 2.302106\n",
      "Epoch 17, loss: 2.299999\n",
      "Epoch 18, loss: 2.300031\n",
      "Epoch 19, loss: 2.301303\n",
      "Epoch 20, loss: 2.300010\n",
      "Epoch 21, loss: 2.300746\n",
      "Epoch 22, loss: 2.299514\n",
      "Epoch 23, loss: 2.299806\n",
      "Epoch 24, loss: 2.301255\n",
      "Epoch 25, loss: 2.301184\n",
      "Epoch 26, loss: 2.301083\n",
      "Epoch 27, loss: 2.299288\n",
      "Epoch 28, loss: 2.301254\n",
      "Epoch 29, loss: 2.300861\n",
      "Epoch 30, loss: 2.299582\n",
      "Epoch 31, loss: 2.296201\n",
      "Epoch 32, loss: 2.299410\n",
      "Epoch 33, loss: 2.299548\n",
      "Epoch 34, loss: 2.299853\n",
      "Epoch 35, loss: 2.298510\n",
      "Epoch 36, loss: 2.300986\n",
      "Epoch 37, loss: 2.297062\n",
      "Epoch 38, loss: 2.301848\n",
      "Epoch 39, loss: 2.298364\n",
      "Epoch 40, loss: 2.298418\n",
      "Epoch 41, loss: 2.297619\n",
      "Epoch 42, loss: 2.300793\n",
      "Epoch 43, loss: 2.299784\n",
      "Epoch 44, loss: 2.297061\n",
      "Epoch 45, loss: 2.297398\n",
      "Epoch 46, loss: 2.297870\n",
      "Epoch 47, loss: 2.301441\n",
      "Epoch 48, loss: 2.299926\n",
      "Epoch 49, loss: 2.298251\n",
      "Epoch 50, loss: 2.298454\n",
      "Epoch 51, loss: 2.296161\n",
      "Epoch 52, loss: 2.297619\n",
      "Epoch 53, loss: 2.295315\n",
      "Epoch 54, loss: 2.296117\n",
      "Epoch 55, loss: 2.297490\n",
      "Epoch 56, loss: 2.300834\n",
      "Epoch 57, loss: 2.298340\n",
      "Epoch 58, loss: 2.299727\n",
      "Epoch 59, loss: 2.296415\n",
      "Epoch 60, loss: 2.296423\n",
      "Epoch 61, loss: 2.297537\n",
      "Epoch 62, loss: 2.298520\n",
      "Epoch 63, loss: 2.297076\n",
      "Epoch 64, loss: 2.298046\n",
      "Epoch 65, loss: 2.296909\n",
      "Epoch 66, loss: 2.297809\n",
      "Epoch 67, loss: 2.296545\n",
      "Epoch 68, loss: 2.298626\n",
      "Epoch 69, loss: 2.295791\n",
      "Epoch 70, loss: 2.297436\n",
      "Epoch 71, loss: 2.297056\n",
      "Epoch 72, loss: 2.299454\n",
      "Epoch 73, loss: 2.296518\n",
      "Epoch 74, loss: 2.294545\n",
      "Epoch 75, loss: 2.296702\n",
      "Epoch 76, loss: 2.295332\n",
      "Epoch 77, loss: 2.299548\n",
      "Epoch 78, loss: 2.297664\n",
      "Epoch 79, loss: 2.296914\n",
      "Epoch 80, loss: 2.293187\n",
      "Epoch 81, loss: 2.293332\n",
      "Epoch 82, loss: 2.298043\n",
      "Epoch 83, loss: 2.297227\n",
      "Epoch 84, loss: 2.293053\n",
      "Epoch 85, loss: 2.298529\n",
      "Epoch 86, loss: 2.295633\n",
      "Epoch 87, loss: 2.295220\n",
      "Epoch 88, loss: 2.291707\n",
      "Epoch 89, loss: 2.296973\n",
      "Epoch 90, loss: 2.299208\n",
      "Epoch 91, loss: 2.292001\n",
      "Epoch 92, loss: 2.296541\n",
      "Epoch 93, loss: 2.293972\n",
      "Epoch 94, loss: 2.295507\n",
      "Epoch 95, loss: 2.297169\n",
      "Epoch 96, loss: 2.296233\n",
      "Epoch 97, loss: 2.292200\n",
      "Epoch 98, loss: 2.291620\n",
      "Epoch 99, loss: 2.295405\n",
      "Epoch 100, loss: 2.292309\n",
      "Epoch 101, loss: 2.295399\n",
      "Epoch 102, loss: 2.294605\n",
      "Epoch 103, loss: 2.293446\n",
      "Epoch 104, loss: 2.291715\n",
      "Epoch 105, loss: 2.293859\n",
      "Epoch 106, loss: 2.294716\n",
      "Epoch 107, loss: 2.293000\n",
      "Epoch 108, loss: 2.294168\n",
      "Epoch 109, loss: 2.293645\n",
      "Epoch 110, loss: 2.293179\n",
      "Epoch 111, loss: 2.292114\n",
      "Epoch 112, loss: 2.290598\n",
      "Epoch 113, loss: 2.295671\n",
      "Epoch 114, loss: 2.295940\n",
      "Epoch 115, loss: 2.295333\n",
      "Epoch 116, loss: 2.294169\n",
      "Epoch 117, loss: 2.292405\n",
      "Epoch 118, loss: 2.294124\n",
      "Epoch 119, loss: 2.291861\n",
      "Epoch 120, loss: 2.289258\n",
      "Epoch 121, loss: 2.293023\n",
      "Epoch 122, loss: 2.289099\n",
      "Epoch 123, loss: 2.289916\n",
      "Epoch 124, loss: 2.290556\n",
      "Epoch 125, loss: 2.294980\n",
      "Epoch 126, loss: 2.290915\n",
      "Epoch 127, loss: 2.293090\n",
      "Epoch 128, loss: 2.291616\n",
      "Epoch 129, loss: 2.286596\n",
      "Epoch 130, loss: 2.290353\n",
      "Epoch 131, loss: 2.294101\n",
      "Epoch 132, loss: 2.295198\n",
      "Epoch 133, loss: 2.291706\n",
      "Epoch 134, loss: 2.293570\n",
      "Epoch 135, loss: 2.288823\n",
      "Epoch 136, loss: 2.291877\n",
      "Epoch 137, loss: 2.293220\n",
      "Epoch 138, loss: 2.287921\n",
      "Epoch 139, loss: 2.288803\n",
      "Epoch 140, loss: 2.289027\n",
      "Epoch 141, loss: 2.290167\n",
      "Epoch 142, loss: 2.291975\n",
      "Epoch 143, loss: 2.288593\n",
      "Epoch 144, loss: 2.289934\n",
      "Epoch 145, loss: 2.291929\n",
      "Epoch 146, loss: 2.290926\n",
      "Epoch 147, loss: 2.287914\n",
      "Epoch 148, loss: 2.288523\n",
      "Epoch 149, loss: 2.289056\n",
      "Epoch 150, loss: 2.289465\n",
      "Epoch 151, loss: 2.289716\n",
      "Epoch 152, loss: 2.291900\n",
      "Epoch 153, loss: 2.290937\n",
      "Epoch 154, loss: 2.293078\n",
      "Epoch 155, loss: 2.290632\n",
      "Epoch 156, loss: 2.291734\n",
      "Epoch 157, loss: 2.286138\n",
      "Epoch 158, loss: 2.288556\n",
      "Epoch 159, loss: 2.291400\n",
      "Epoch 160, loss: 2.289168\n",
      "Epoch 161, loss: 2.286022\n",
      "Epoch 162, loss: 2.290588\n",
      "Epoch 163, loss: 2.286069\n",
      "Epoch 164, loss: 2.288494\n",
      "Epoch 165, loss: 2.286134\n",
      "Epoch 166, loss: 2.286604\n",
      "Epoch 167, loss: 2.291361\n",
      "Epoch 168, loss: 2.288913\n",
      "Epoch 169, loss: 2.288378\n",
      "Epoch 170, loss: 2.288802\n",
      "Epoch 171, loss: 2.287537\n",
      "Epoch 172, loss: 2.288471\n",
      "Epoch 173, loss: 2.284223\n",
      "Epoch 174, loss: 2.288753\n",
      "Epoch 175, loss: 2.294731\n",
      "Epoch 176, loss: 2.290192\n",
      "Epoch 177, loss: 2.285994\n",
      "Epoch 178, loss: 2.284998\n",
      "Epoch 179, loss: 2.289742\n",
      "Epoch 180, loss: 2.288961\n",
      "Epoch 181, loss: 2.288148\n",
      "Epoch 182, loss: 2.283735\n",
      "Epoch 183, loss: 2.285563\n",
      "Epoch 184, loss: 2.289516\n",
      "Epoch 185, loss: 2.288493\n",
      "Epoch 186, loss: 2.290049\n",
      "Epoch 187, loss: 2.292508\n",
      "Epoch 188, loss: 2.284327\n",
      "Epoch 189, loss: 2.283179\n",
      "Epoch 190, loss: 2.289289\n",
      "Epoch 191, loss: 2.281365\n",
      "Epoch 192, loss: 2.284827\n",
      "Epoch 193, loss: 2.290639\n",
      "Epoch 194, loss: 2.288525\n",
      "Epoch 195, loss: 2.290395\n",
      "Epoch 196, loss: 2.292643\n",
      "Epoch 197, loss: 2.287388\n",
      "Epoch 198, loss: 2.285921\n",
      "Epoch 199, loss: 2.288010\n",
      "Epoch 0, loss: 2.301952\n",
      "Epoch 1, loss: 2.303154\n",
      "Epoch 2, loss: 2.303059\n",
      "Epoch 3, loss: 2.303015\n",
      "Epoch 4, loss: 2.302378\n",
      "Epoch 5, loss: 2.302294\n",
      "Epoch 6, loss: 2.302590\n",
      "Epoch 7, loss: 2.302388\n",
      "Epoch 8, loss: 2.301715\n",
      "Epoch 9, loss: 2.302131\n",
      "Epoch 10, loss: 2.302166\n",
      "Epoch 11, loss: 2.302307\n",
      "Epoch 12, loss: 2.303234\n",
      "Epoch 13, loss: 2.302261\n",
      "Epoch 14, loss: 2.303275\n",
      "Epoch 15, loss: 2.302395\n",
      "Epoch 16, loss: 2.301973\n",
      "Epoch 17, loss: 2.301959\n",
      "Epoch 18, loss: 2.301965\n",
      "Epoch 19, loss: 2.302160\n",
      "Epoch 20, loss: 2.301881\n",
      "Epoch 21, loss: 2.302125\n",
      "Epoch 22, loss: 2.303297\n",
      "Epoch 23, loss: 2.302282\n",
      "Epoch 24, loss: 2.302379\n",
      "Epoch 25, loss: 2.301778\n",
      "Epoch 26, loss: 2.303302\n",
      "Epoch 27, loss: 2.301717\n",
      "Epoch 28, loss: 2.302510\n",
      "Epoch 29, loss: 2.301629\n",
      "Epoch 30, loss: 2.302321\n",
      "Epoch 31, loss: 2.301850\n",
      "Epoch 32, loss: 2.303837\n",
      "Epoch 33, loss: 2.303038\n",
      "Epoch 34, loss: 2.302096\n",
      "Epoch 35, loss: 2.301442\n",
      "Epoch 36, loss: 2.303111\n",
      "Epoch 37, loss: 2.302447\n",
      "Epoch 38, loss: 2.302877\n",
      "Epoch 39, loss: 2.301463\n",
      "Epoch 40, loss: 2.303258\n",
      "Epoch 41, loss: 2.301665\n",
      "Epoch 42, loss: 2.302075\n",
      "Epoch 43, loss: 2.301634\n",
      "Epoch 44, loss: 2.302537\n",
      "Epoch 45, loss: 2.301656\n",
      "Epoch 46, loss: 2.301971\n",
      "Epoch 47, loss: 2.302356\n",
      "Epoch 48, loss: 2.301934\n",
      "Epoch 49, loss: 2.302028\n",
      "Epoch 50, loss: 2.301937\n",
      "Epoch 51, loss: 2.301345\n",
      "Epoch 52, loss: 2.301736\n",
      "Epoch 53, loss: 2.301776\n",
      "Epoch 54, loss: 2.302230\n",
      "Epoch 55, loss: 2.302109\n",
      "Epoch 56, loss: 2.302861\n",
      "Epoch 57, loss: 2.302317\n",
      "Epoch 58, loss: 2.303266\n",
      "Epoch 59, loss: 2.301930\n",
      "Epoch 60, loss: 2.302435\n",
      "Epoch 61, loss: 2.302075\n",
      "Epoch 62, loss: 2.301542\n",
      "Epoch 63, loss: 2.301227\n",
      "Epoch 64, loss: 2.301670\n",
      "Epoch 65, loss: 2.302201\n",
      "Epoch 66, loss: 2.301827\n",
      "Epoch 67, loss: 2.300534\n",
      "Epoch 68, loss: 2.301944\n",
      "Epoch 69, loss: 2.301540\n",
      "Epoch 70, loss: 2.301797\n",
      "Epoch 71, loss: 2.302188\n",
      "Epoch 72, loss: 2.302514\n",
      "Epoch 73, loss: 2.302225\n",
      "Epoch 74, loss: 2.301206\n",
      "Epoch 75, loss: 2.302046\n",
      "Epoch 76, loss: 2.302070\n",
      "Epoch 77, loss: 2.301500\n",
      "Epoch 78, loss: 2.302167\n",
      "Epoch 79, loss: 2.301124\n",
      "Epoch 80, loss: 2.302117\n",
      "Epoch 81, loss: 2.300800\n",
      "Epoch 82, loss: 2.302257\n",
      "Epoch 83, loss: 2.301439\n",
      "Epoch 84, loss: 2.302812\n",
      "Epoch 85, loss: 2.301755\n",
      "Epoch 86, loss: 2.302354\n",
      "Epoch 87, loss: 2.301368\n",
      "Epoch 88, loss: 2.301882\n",
      "Epoch 89, loss: 2.301708\n",
      "Epoch 90, loss: 2.301994\n",
      "Epoch 91, loss: 2.301565\n",
      "Epoch 92, loss: 2.302022\n",
      "Epoch 93, loss: 2.301061\n",
      "Epoch 94, loss: 2.302359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, loss: 2.301603\n",
      "Epoch 96, loss: 2.300768\n",
      "Epoch 97, loss: 2.302235\n",
      "Epoch 98, loss: 2.301541\n",
      "Epoch 99, loss: 2.302070\n",
      "Epoch 100, loss: 2.301013\n",
      "Epoch 101, loss: 2.301631\n",
      "Epoch 102, loss: 2.301555\n",
      "Epoch 103, loss: 2.300567\n",
      "Epoch 104, loss: 2.301724\n",
      "Epoch 105, loss: 2.302443\n",
      "Epoch 106, loss: 2.301831\n",
      "Epoch 107, loss: 2.301877\n",
      "Epoch 108, loss: 2.302070\n",
      "Epoch 109, loss: 2.302094\n",
      "Epoch 110, loss: 2.301200\n",
      "Epoch 111, loss: 2.300394\n",
      "Epoch 112, loss: 2.301580\n",
      "Epoch 113, loss: 2.301938\n",
      "Epoch 114, loss: 2.300243\n",
      "Epoch 115, loss: 2.301096\n",
      "Epoch 116, loss: 2.300993\n",
      "Epoch 117, loss: 2.301207\n",
      "Epoch 118, loss: 2.301594\n",
      "Epoch 119, loss: 2.301730\n",
      "Epoch 120, loss: 2.301931\n",
      "Epoch 121, loss: 2.300632\n",
      "Epoch 122, loss: 2.302252\n",
      "Epoch 123, loss: 2.302188\n",
      "Epoch 124, loss: 2.301012\n",
      "Epoch 125, loss: 2.302560\n",
      "Epoch 126, loss: 2.301908\n",
      "Epoch 127, loss: 2.302056\n",
      "Epoch 128, loss: 2.302055\n",
      "Epoch 129, loss: 2.302546\n",
      "Epoch 130, loss: 2.301308\n",
      "Epoch 131, loss: 2.301329\n",
      "Epoch 132, loss: 2.301485\n",
      "Epoch 133, loss: 2.300069\n",
      "Epoch 134, loss: 2.300172\n",
      "Epoch 135, loss: 2.301767\n",
      "Epoch 136, loss: 2.301689\n",
      "Epoch 137, loss: 2.301626\n",
      "Epoch 138, loss: 2.302402\n",
      "Epoch 139, loss: 2.300710\n",
      "Epoch 140, loss: 2.301098\n",
      "Epoch 141, loss: 2.301075\n",
      "Epoch 142, loss: 2.301475\n",
      "Epoch 143, loss: 2.301114\n",
      "Epoch 144, loss: 2.301918\n",
      "Epoch 145, loss: 2.300765\n",
      "Epoch 146, loss: 2.301445\n",
      "Epoch 147, loss: 2.303044\n",
      "Epoch 148, loss: 2.301524\n",
      "Epoch 149, loss: 2.301956\n",
      "Epoch 150, loss: 2.299381\n",
      "Epoch 151, loss: 2.300340\n",
      "Epoch 152, loss: 2.302179\n",
      "Epoch 153, loss: 2.302697\n",
      "Epoch 154, loss: 2.300845\n",
      "Epoch 155, loss: 2.300734\n",
      "Epoch 156, loss: 2.300926\n",
      "Epoch 157, loss: 2.300822\n",
      "Epoch 158, loss: 2.300283\n",
      "Epoch 159, loss: 2.300603\n",
      "Epoch 160, loss: 2.300234\n",
      "Epoch 161, loss: 2.300786\n",
      "Epoch 162, loss: 2.301916\n",
      "Epoch 163, loss: 2.301752\n",
      "Epoch 164, loss: 2.301352\n",
      "Epoch 165, loss: 2.301849\n",
      "Epoch 166, loss: 2.301330\n",
      "Epoch 167, loss: 2.301876\n",
      "Epoch 168, loss: 2.300677\n",
      "Epoch 169, loss: 2.301726\n",
      "Epoch 170, loss: 2.299998\n",
      "Epoch 171, loss: 2.299731\n",
      "Epoch 172, loss: 2.299842\n",
      "Epoch 173, loss: 2.300638\n",
      "Epoch 174, loss: 2.301403\n",
      "Epoch 175, loss: 2.301773\n",
      "Epoch 176, loss: 2.299434\n",
      "Epoch 177, loss: 2.300436\n",
      "Epoch 178, loss: 2.301282\n",
      "Epoch 179, loss: 2.300978\n",
      "Epoch 180, loss: 2.300630\n",
      "Epoch 181, loss: 2.300468\n",
      "Epoch 182, loss: 2.302239\n",
      "Epoch 183, loss: 2.300786\n",
      "Epoch 184, loss: 2.300878\n",
      "Epoch 185, loss: 2.300593\n",
      "Epoch 186, loss: 2.300352\n",
      "Epoch 187, loss: 2.301480\n",
      "Epoch 188, loss: 2.301727\n",
      "Epoch 189, loss: 2.301510\n",
      "Epoch 190, loss: 2.299506\n",
      "Epoch 191, loss: 2.300726\n",
      "Epoch 192, loss: 2.300071\n",
      "Epoch 193, loss: 2.299350\n",
      "Epoch 194, loss: 2.300001\n",
      "Epoch 195, loss: 2.302193\n",
      "Epoch 196, loss: 2.299045\n",
      "Epoch 197, loss: 2.302195\n",
      "Epoch 198, loss: 2.300010\n",
      "Epoch 199, loss: 2.299330\n",
      "Epoch 0, loss: 2.303410\n",
      "Epoch 1, loss: 2.301701\n",
      "Epoch 2, loss: 2.302889\n",
      "Epoch 3, loss: 2.302715\n",
      "Epoch 4, loss: 2.302111\n",
      "Epoch 5, loss: 2.302762\n",
      "Epoch 6, loss: 2.301600\n",
      "Epoch 7, loss: 2.302787\n",
      "Epoch 8, loss: 2.301665\n",
      "Epoch 9, loss: 2.301518\n",
      "Epoch 10, loss: 2.302998\n",
      "Epoch 11, loss: 2.301983\n",
      "Epoch 12, loss: 2.301575\n",
      "Epoch 13, loss: 2.302463\n",
      "Epoch 14, loss: 2.302967\n",
      "Epoch 15, loss: 2.302625\n",
      "Epoch 16, loss: 2.303002\n",
      "Epoch 17, loss: 2.302432\n",
      "Epoch 18, loss: 2.302770\n",
      "Epoch 19, loss: 2.301435\n",
      "Epoch 20, loss: 2.302406\n",
      "Epoch 21, loss: 2.301966\n",
      "Epoch 22, loss: 2.302095\n",
      "Epoch 23, loss: 2.302241\n",
      "Epoch 24, loss: 2.301621\n",
      "Epoch 25, loss: 2.302415\n",
      "Epoch 26, loss: 2.302511\n",
      "Epoch 27, loss: 2.302845\n",
      "Epoch 28, loss: 2.302057\n",
      "Epoch 29, loss: 2.302060\n",
      "Epoch 30, loss: 2.301717\n",
      "Epoch 31, loss: 2.302360\n",
      "Epoch 32, loss: 2.301457\n",
      "Epoch 33, loss: 2.301571\n",
      "Epoch 34, loss: 2.300712\n",
      "Epoch 35, loss: 2.302310\n",
      "Epoch 36, loss: 2.302086\n",
      "Epoch 37, loss: 2.302557\n",
      "Epoch 38, loss: 2.301204\n",
      "Epoch 39, loss: 2.302018\n",
      "Epoch 40, loss: 2.301555\n",
      "Epoch 41, loss: 2.302264\n",
      "Epoch 42, loss: 2.301217\n",
      "Epoch 43, loss: 2.300940\n",
      "Epoch 44, loss: 2.303149\n",
      "Epoch 45, loss: 2.302930\n",
      "Epoch 46, loss: 2.301732\n",
      "Epoch 47, loss: 2.301687\n",
      "Epoch 48, loss: 2.301493\n",
      "Epoch 49, loss: 2.301741\n",
      "Epoch 50, loss: 2.300575\n",
      "Epoch 51, loss: 2.302333\n",
      "Epoch 52, loss: 2.302470\n",
      "Epoch 53, loss: 2.301455\n",
      "Epoch 54, loss: 2.300943\n",
      "Epoch 55, loss: 2.302293\n",
      "Epoch 56, loss: 2.302573\n",
      "Epoch 57, loss: 2.302167\n",
      "Epoch 58, loss: 2.301651\n",
      "Epoch 59, loss: 2.302487\n",
      "Epoch 60, loss: 2.301923\n",
      "Epoch 61, loss: 2.302312\n",
      "Epoch 62, loss: 2.301421\n",
      "Epoch 63, loss: 2.302365\n",
      "Epoch 64, loss: 2.302752\n",
      "Epoch 65, loss: 2.302135\n",
      "Epoch 66, loss: 2.302253\n",
      "Epoch 67, loss: 2.302807\n",
      "Epoch 68, loss: 2.301774\n",
      "Epoch 69, loss: 2.302667\n",
      "Epoch 70, loss: 2.301490\n",
      "Epoch 71, loss: 2.301737\n",
      "Epoch 72, loss: 2.302223\n",
      "Epoch 73, loss: 2.301977\n",
      "Epoch 74, loss: 2.302229\n",
      "Epoch 75, loss: 2.303216\n",
      "Epoch 76, loss: 2.301951\n",
      "Epoch 77, loss: 2.301115\n",
      "Epoch 78, loss: 2.301992\n",
      "Epoch 79, loss: 2.302698\n",
      "Epoch 80, loss: 2.301768\n",
      "Epoch 81, loss: 2.302370\n",
      "Epoch 82, loss: 2.301053\n",
      "Epoch 83, loss: 2.300377\n",
      "Epoch 84, loss: 2.300863\n",
      "Epoch 85, loss: 2.302729\n",
      "Epoch 86, loss: 2.301923\n",
      "Epoch 87, loss: 2.302316\n",
      "Epoch 88, loss: 2.301662\n",
      "Epoch 89, loss: 2.301342\n",
      "Epoch 90, loss: 2.301947\n",
      "Epoch 91, loss: 2.301144\n",
      "Epoch 92, loss: 2.301570\n",
      "Epoch 93, loss: 2.300898\n",
      "Epoch 94, loss: 2.301789\n",
      "Epoch 95, loss: 2.301424\n",
      "Epoch 96, loss: 2.302591\n",
      "Epoch 97, loss: 2.302074\n",
      "Epoch 98, loss: 2.300643\n",
      "Epoch 99, loss: 2.301269\n",
      "Epoch 100, loss: 2.302289\n",
      "Epoch 101, loss: 2.301655\n",
      "Epoch 102, loss: 2.301783\n",
      "Epoch 103, loss: 2.302289\n",
      "Epoch 104, loss: 2.301009\n",
      "Epoch 105, loss: 2.302040\n",
      "Epoch 106, loss: 2.301916\n",
      "Epoch 107, loss: 2.301577\n",
      "Epoch 108, loss: 2.301347\n",
      "Epoch 109, loss: 2.301609\n",
      "Epoch 110, loss: 2.299838\n",
      "Epoch 111, loss: 2.302273\n",
      "Epoch 112, loss: 2.302466\n",
      "Epoch 113, loss: 2.301898\n",
      "Epoch 114, loss: 2.302111\n",
      "Epoch 115, loss: 2.302640\n",
      "Epoch 116, loss: 2.301242\n",
      "Epoch 117, loss: 2.302415\n",
      "Epoch 118, loss: 2.301528\n",
      "Epoch 119, loss: 2.301876\n",
      "Epoch 120, loss: 2.302622\n",
      "Epoch 121, loss: 2.301604\n",
      "Epoch 122, loss: 2.300604\n",
      "Epoch 123, loss: 2.301532\n",
      "Epoch 124, loss: 2.300780\n",
      "Epoch 125, loss: 2.301896\n",
      "Epoch 126, loss: 2.299909\n",
      "Epoch 127, loss: 2.300896\n",
      "Epoch 128, loss: 2.302187\n",
      "Epoch 129, loss: 2.300890\n",
      "Epoch 130, loss: 2.301944\n",
      "Epoch 131, loss: 2.301254\n",
      "Epoch 132, loss: 2.300418\n",
      "Epoch 133, loss: 2.302559\n",
      "Epoch 134, loss: 2.300818\n",
      "Epoch 135, loss: 2.300724\n",
      "Epoch 136, loss: 2.301646\n",
      "Epoch 137, loss: 2.299989\n",
      "Epoch 138, loss: 2.302077\n",
      "Epoch 139, loss: 2.301780\n",
      "Epoch 140, loss: 2.300817\n",
      "Epoch 141, loss: 2.301316\n",
      "Epoch 142, loss: 2.302106\n",
      "Epoch 143, loss: 2.302142\n",
      "Epoch 144, loss: 2.300374\n",
      "Epoch 145, loss: 2.301200\n",
      "Epoch 146, loss: 2.300183\n",
      "Epoch 147, loss: 2.301384\n",
      "Epoch 148, loss: 2.302202\n",
      "Epoch 149, loss: 2.301404\n",
      "Epoch 150, loss: 2.300276\n",
      "Epoch 151, loss: 2.301292\n",
      "Epoch 152, loss: 2.300969\n",
      "Epoch 153, loss: 2.300809\n",
      "Epoch 154, loss: 2.301090\n",
      "Epoch 155, loss: 2.301369\n",
      "Epoch 156, loss: 2.303387\n",
      "Epoch 157, loss: 2.301931\n",
      "Epoch 158, loss: 2.300881\n",
      "Epoch 159, loss: 2.301952\n",
      "Epoch 160, loss: 2.300530\n",
      "Epoch 161, loss: 2.301979\n",
      "Epoch 162, loss: 2.299390\n",
      "Epoch 163, loss: 2.301606\n",
      "Epoch 164, loss: 2.300744\n",
      "Epoch 165, loss: 2.300392\n",
      "Epoch 166, loss: 2.302395\n",
      "Epoch 167, loss: 2.301492\n",
      "Epoch 168, loss: 2.300040\n",
      "Epoch 169, loss: 2.301150\n",
      "Epoch 170, loss: 2.300793\n",
      "Epoch 171, loss: 2.302659\n",
      "Epoch 172, loss: 2.301191\n",
      "Epoch 173, loss: 2.302053\n",
      "Epoch 174, loss: 2.300442\n",
      "Epoch 175, loss: 2.300763\n",
      "Epoch 176, loss: 2.302093\n",
      "Epoch 177, loss: 2.300253\n",
      "Epoch 178, loss: 2.300865\n",
      "Epoch 179, loss: 2.300428\n",
      "Epoch 180, loss: 2.301520\n",
      "Epoch 181, loss: 2.300692\n",
      "Epoch 182, loss: 2.301644\n",
      "Epoch 183, loss: 2.300574\n",
      "Epoch 184, loss: 2.301305\n",
      "Epoch 185, loss: 2.301459\n",
      "Epoch 186, loss: 2.300522\n",
      "Epoch 187, loss: 2.300954\n",
      "Epoch 188, loss: 2.301146\n",
      "Epoch 189, loss: 2.299795\n",
      "Epoch 190, loss: 2.300878\n",
      "Epoch 191, loss: 2.300504\n",
      "Epoch 192, loss: 2.300562\n",
      "Epoch 193, loss: 2.300816\n",
      "Epoch 194, loss: 2.300310\n",
      "Epoch 195, loss: 2.300807\n",
      "Epoch 196, loss: 2.301486\n",
      "Epoch 197, loss: 2.302947\n",
      "Epoch 198, loss: 2.299089\n",
      "Epoch 199, loss: 2.300343\n",
      "Epoch 0, loss: 2.302952\n",
      "Epoch 1, loss: 2.302010\n",
      "Epoch 2, loss: 2.303895\n",
      "Epoch 3, loss: 2.302757\n",
      "Epoch 4, loss: 2.303192\n",
      "Epoch 5, loss: 2.303596\n",
      "Epoch 6, loss: 2.301580\n",
      "Epoch 7, loss: 2.301622\n",
      "Epoch 8, loss: 2.301726\n",
      "Epoch 9, loss: 2.302631\n",
      "Epoch 10, loss: 2.302724\n",
      "Epoch 11, loss: 2.303304\n",
      "Epoch 12, loss: 2.302086\n",
      "Epoch 13, loss: 2.301959\n",
      "Epoch 14, loss: 2.301821\n",
      "Epoch 15, loss: 2.302805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, loss: 2.302778\n",
      "Epoch 17, loss: 2.302333\n",
      "Epoch 18, loss: 2.302344\n",
      "Epoch 19, loss: 2.303686\n",
      "Epoch 20, loss: 2.302745\n",
      "Epoch 21, loss: 2.301854\n",
      "Epoch 22, loss: 2.302344\n",
      "Epoch 23, loss: 2.302606\n",
      "Epoch 24, loss: 2.302626\n",
      "Epoch 25, loss: 2.302579\n",
      "Epoch 26, loss: 2.303869\n",
      "Epoch 27, loss: 2.301406\n",
      "Epoch 28, loss: 2.302414\n",
      "Epoch 29, loss: 2.303277\n",
      "Epoch 30, loss: 2.302953\n",
      "Epoch 31, loss: 2.301824\n",
      "Epoch 32, loss: 2.303059\n",
      "Epoch 33, loss: 2.303106\n",
      "Epoch 34, loss: 2.301722\n",
      "Epoch 35, loss: 2.301817\n",
      "Epoch 36, loss: 2.302659\n",
      "Epoch 37, loss: 2.302812\n",
      "Epoch 38, loss: 2.302088\n",
      "Epoch 39, loss: 2.301499\n",
      "Epoch 40, loss: 2.302864\n",
      "Epoch 41, loss: 2.303001\n",
      "Epoch 42, loss: 2.302215\n",
      "Epoch 43, loss: 2.302078\n",
      "Epoch 44, loss: 2.302419\n",
      "Epoch 45, loss: 2.302230\n",
      "Epoch 46, loss: 2.301410\n",
      "Epoch 47, loss: 2.302163\n",
      "Epoch 48, loss: 2.302751\n",
      "Epoch 49, loss: 2.302483\n",
      "Epoch 50, loss: 2.302593\n",
      "Epoch 51, loss: 2.303014\n",
      "Epoch 52, loss: 2.302865\n",
      "Epoch 53, loss: 2.301986\n",
      "Epoch 54, loss: 2.302403\n",
      "Epoch 55, loss: 2.301565\n",
      "Epoch 56, loss: 2.302077\n",
      "Epoch 57, loss: 2.302327\n",
      "Epoch 58, loss: 2.302173\n",
      "Epoch 59, loss: 2.301220\n",
      "Epoch 60, loss: 2.302336\n",
      "Epoch 61, loss: 2.302079\n",
      "Epoch 62, loss: 2.302042\n",
      "Epoch 63, loss: 2.301479\n",
      "Epoch 64, loss: 2.302047\n",
      "Epoch 65, loss: 2.301962\n",
      "Epoch 66, loss: 2.301444\n",
      "Epoch 67, loss: 2.302201\n",
      "Epoch 68, loss: 2.302385\n",
      "Epoch 69, loss: 2.301877\n",
      "Epoch 70, loss: 2.300889\n",
      "Epoch 71, loss: 2.301717\n",
      "Epoch 72, loss: 2.300824\n",
      "Epoch 73, loss: 2.301432\n",
      "Epoch 74, loss: 2.302787\n",
      "Epoch 75, loss: 2.302649\n",
      "Epoch 76, loss: 2.301690\n",
      "Epoch 77, loss: 2.301558\n",
      "Epoch 78, loss: 2.303048\n",
      "Epoch 79, loss: 2.301598\n",
      "Epoch 80, loss: 2.301183\n",
      "Epoch 81, loss: 2.302010\n",
      "Epoch 82, loss: 2.301872\n",
      "Epoch 83, loss: 2.302562\n",
      "Epoch 84, loss: 2.302580\n",
      "Epoch 85, loss: 2.302512\n",
      "Epoch 86, loss: 2.301907\n",
      "Epoch 87, loss: 2.301769\n",
      "Epoch 88, loss: 2.302269\n",
      "Epoch 89, loss: 2.301370\n",
      "Epoch 90, loss: 2.303325\n",
      "Epoch 91, loss: 2.301408\n",
      "Epoch 92, loss: 2.300922\n",
      "Epoch 93, loss: 2.301616\n",
      "Epoch 94, loss: 2.301139\n",
      "Epoch 95, loss: 2.301374\n",
      "Epoch 96, loss: 2.302437\n",
      "Epoch 97, loss: 2.301236\n",
      "Epoch 98, loss: 2.301316\n",
      "Epoch 99, loss: 2.301609\n",
      "Epoch 100, loss: 2.300728\n",
      "Epoch 101, loss: 2.301931\n",
      "Epoch 102, loss: 2.300706\n",
      "Epoch 103, loss: 2.300944\n",
      "Epoch 104, loss: 2.302590\n",
      "Epoch 105, loss: 2.301471\n",
      "Epoch 106, loss: 2.301735\n",
      "Epoch 107, loss: 2.302710\n",
      "Epoch 108, loss: 2.301868\n",
      "Epoch 109, loss: 2.300726\n",
      "Epoch 110, loss: 2.301919\n",
      "Epoch 111, loss: 2.302258\n",
      "Epoch 112, loss: 2.302349\n",
      "Epoch 113, loss: 2.301257\n",
      "Epoch 114, loss: 2.302056\n",
      "Epoch 115, loss: 2.300534\n",
      "Epoch 116, loss: 2.302441\n",
      "Epoch 117, loss: 2.300895\n",
      "Epoch 118, loss: 2.301030\n",
      "Epoch 119, loss: 2.302914\n",
      "Epoch 120, loss: 2.301553\n",
      "Epoch 121, loss: 2.302306\n",
      "Epoch 122, loss: 2.301144\n",
      "Epoch 123, loss: 2.301436\n",
      "Epoch 124, loss: 2.301300\n",
      "Epoch 125, loss: 2.300354\n",
      "Epoch 126, loss: 2.300769\n",
      "Epoch 127, loss: 2.301901\n",
      "Epoch 128, loss: 2.300736\n",
      "Epoch 129, loss: 2.302071\n",
      "Epoch 130, loss: 2.302034\n",
      "Epoch 131, loss: 2.301162\n",
      "Epoch 132, loss: 2.301522\n",
      "Epoch 133, loss: 2.300103\n",
      "Epoch 134, loss: 2.301867\n",
      "Epoch 135, loss: 2.301912\n",
      "Epoch 136, loss: 2.301276\n",
      "Epoch 137, loss: 2.302688\n",
      "Epoch 138, loss: 2.302366\n",
      "Epoch 139, loss: 2.300551\n",
      "Epoch 140, loss: 2.302707\n",
      "Epoch 141, loss: 2.302009\n",
      "Epoch 142, loss: 2.300865\n",
      "Epoch 143, loss: 2.301326\n",
      "Epoch 144, loss: 2.301724\n",
      "Epoch 145, loss: 2.301999\n",
      "Epoch 146, loss: 2.301299\n",
      "Epoch 147, loss: 2.301239\n",
      "Epoch 148, loss: 2.299926\n",
      "Epoch 149, loss: 2.301168\n",
      "Epoch 150, loss: 2.300611\n",
      "Epoch 151, loss: 2.301792\n",
      "Epoch 152, loss: 2.301259\n",
      "Epoch 153, loss: 2.301621\n",
      "Epoch 154, loss: 2.300429\n",
      "Epoch 155, loss: 2.301347\n",
      "Epoch 156, loss: 2.300959\n",
      "Epoch 157, loss: 2.301485\n",
      "Epoch 158, loss: 2.301467\n",
      "Epoch 159, loss: 2.302214\n",
      "Epoch 160, loss: 2.301493\n",
      "Epoch 161, loss: 2.302136\n",
      "Epoch 162, loss: 2.300938\n",
      "Epoch 163, loss: 2.301574\n",
      "Epoch 164, loss: 2.301874\n",
      "Epoch 165, loss: 2.301125\n",
      "Epoch 166, loss: 2.300179\n",
      "Epoch 167, loss: 2.301873\n",
      "Epoch 168, loss: 2.300935\n",
      "Epoch 169, loss: 2.302595\n",
      "Epoch 170, loss: 2.300992\n",
      "Epoch 171, loss: 2.301841\n",
      "Epoch 172, loss: 2.299727\n",
      "Epoch 173, loss: 2.302523\n",
      "Epoch 174, loss: 2.300632\n",
      "Epoch 175, loss: 2.300937\n",
      "Epoch 176, loss: 2.301990\n",
      "Epoch 177, loss: 2.300828\n",
      "Epoch 178, loss: 2.301321\n",
      "Epoch 179, loss: 2.301959\n",
      "Epoch 180, loss: 2.300476\n",
      "Epoch 181, loss: 2.300868\n",
      "Epoch 182, loss: 2.301465\n",
      "Epoch 183, loss: 2.301518\n",
      "Epoch 184, loss: 2.302133\n",
      "Epoch 185, loss: 2.301418\n",
      "Epoch 186, loss: 2.299634\n",
      "Epoch 187, loss: 2.299812\n",
      "Epoch 188, loss: 2.301734\n",
      "Epoch 189, loss: 2.300565\n",
      "Epoch 190, loss: 2.300720\n",
      "Epoch 191, loss: 2.301515\n",
      "Epoch 192, loss: 2.301089\n",
      "Epoch 193, loss: 2.300570\n",
      "Epoch 194, loss: 2.301476\n",
      "Epoch 195, loss: 2.300515\n",
      "Epoch 196, loss: 2.300343\n",
      "Epoch 197, loss: 2.301624\n",
      "Epoch 198, loss: 2.301485\n",
      "Epoch 199, loss: 2.300238\n",
      "CPU times: user 15min 31s, sys: 59.5 s, total: 16min 31s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "best_hyper = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "            best_hyper = [lr, rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.225000\n"
     ]
    }
   ],
   "source": [
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 1e-05]"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.217000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
